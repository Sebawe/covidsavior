{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modelTest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpo26QaNYmw0",
        "colab_type": "text"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2LGcEZOWl67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -- coding:utf-8 --\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from os.path import join as pjoin\n",
        "\n",
        "# from utils import is_number\n",
        "\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import tree\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "import xgboost as xgb\n",
        "from xgboost import plot_tree\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "#import utils\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib as mpl"
      ],
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4Isd3DXGIwE",
        "colab_type": "text"
      },
      "source": [
        "# Global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72XGH4LKGHod",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Common parameters\n",
        "top3_feats_cols = ['LDH', 'CRP', 'lymph']\n",
        "in_out_time_cols = ['Admission time', 'Discharge time']\n",
        "train_excel_path = 'time_series_375_prerpocess_en.xlsx'  \n",
        "test_excel_path = 'time_series_test_110_preprocess_en.xlsx'\n",
        "csv_output_path = '/content/predictions.csv'"
      ],
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64i4mOUjY0Gj",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdEFwYHWiK43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def is_number(s):\n",
        "    if s is None:\n",
        "        s = np.nan\n",
        "\n",
        "    try:\n",
        "        float(s)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        import unicodedata\n",
        "        unicodedata.numeric(s)\n",
        "        return True\n",
        "    except (TypeError, ValueError):\n",
        "        pass\n",
        "\n",
        "    return False\n",
        "def read(path: str, usecols=None, is_ts='infer'):\n",
        "    \"\"\"Read the processed data\n",
        "    Combine the read function of parquet, csv, excel files\n",
        "    :param path: File path, must be a parquet or csv or excel file\n",
        "    :param usecols: The selected row. Unlike the pandas interface, it is simplified here, no need to write index columns\n",
        "    :param is_ts: Whether it is a time series. Optional values:'infer', True, False\n",
        "    :return: DateFrame data read\n",
        "    \"\"\"\n",
        "    # Set index\n",
        "    if is_ts == 'infer':\n",
        "        index_col = [0, 1] if os.path.split(path)[1].startswith('time_series') else [0]\n",
        "    elif is_ts is True:\n",
        "        index_col = [0, 1]\n",
        "    elif is_ts is False:\n",
        "        index_col = [0]\n",
        "    else:\n",
        "        raise Exception('is_ts 参数错误')\n",
        "\n",
        "    # Read data\n",
        "    if path.endswith('.parquet'):\n",
        "        data = pd.read_parquet(path)\n",
        "    elif path.endswith('.csv'):\n",
        "        try:\n",
        "            data = pd.read_csv(path, index_col=index_col, encoding='gb18030')\n",
        "        except UnicodeDecodeError:\n",
        "            data = pd.read_csv(path, index_col=index_col, encoding='utf-8')\n",
        "        except:\n",
        "            raise\n",
        "    elif path.endswith('.xlsx'):\n",
        "        data = pd.read_excel(path, index_col=index_col)\n",
        "    else:\n",
        "        raise Exception('File type error')\n",
        "\n",
        "    # Extract the specified column\n",
        "    if usecols is not None:\n",
        "        data = data[usecols]\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def merge_data_by_sliding_window(data, n_days=1, dropna=True, subset=None, time_form='diff'):\n",
        "    \"\"\"Sliding window to merge data\n",
        "    :param data: Time series data, the primary row index is PATIENT_ID, and the secondary row index is RE_DATE\n",
        "    :param n_days: Window length\n",
        "    :param dropna: Whether to delete the missing sliding window after merging\n",
        "    :param subset: pd.DataFrame().dropna() parameter                                                  Note: New parameter!\n",
        "    :param time_form: Return the time index of the data,'diff' or 'timestamp'\n",
        "    :return: For the merged data, the primary row index is PATIENT_ID, and\n",
        "             the secondary row index is t_diff or RE_DATE, depending on \"time_form\"\n",
        "    \"\"\"\n",
        "    #Sort by PATIENT_ID\n",
        "    data = data.reset_index(level=1)\n",
        "    # dt.normalize() Number of days taken out of hospital\n",
        "    # Time to discharge\n",
        "    # Note: The hours, minutes and seconds of discharge time and testing time have been removed,\n",
        "    #       because I think it’s more appropriate to use 00:00:00 as the dividing point\n",
        "    t_diff = data['Discharge time'].dt.normalize() - data['RE_DATE'].dt.normalize()\n",
        "    # The basis for rounding the sliding window. That is, the nn_days days will be rounded to the same value,\n",
        "    # and then grouped by the groupby method\n",
        "    data['t_diff'] = t_diff.dt.days.values // n_days * n_days\n",
        "    #\n",
        "    data = data.set_index('t_diff', append=True)\n",
        "\n",
        "    # Sliding windows merge. For ['PATIENT_ID','t_diff'] groupby,\n",
        "    #   it is equivalent to a double loop. Traverse all patients and all windows of patients\n",
        "    # Because the data was sorted before, each patient's t_diff will be sorted from largest to smallest,\n",
        "    #   and ffill() is the interpolation to the upper row, so it is equivalent to the interpolation to the old date\n",
        "    # last()It is the last row of each group,\n",
        "    #   so the last data of the corresponding window for each patient is taken (it must be the most complete).\n",
        "    # last() comes with sorting. After taking last, it will be sorted in ascending order of index\n",
        "    data = (\n",
        "        data\n",
        "        # Fill the latest variable of everything\n",
        "        .groupby(['PATIENT_ID', 't_diff']).ffill()\n",
        "        # Select the latest row (newest)\n",
        "        .groupby(['PATIENT_ID', 't_diff']).last()\n",
        "    )\n",
        "    # Remove missing samples\n",
        "    if dropna:\n",
        "        data = data.dropna(subset=subset)         # Note: Here dropna() is performed on missing values instead of fillna(-1)\n",
        "\n",
        "    # Update the secondary index. (Actually, timestamp is not used in this paper)\n",
        "    if time_form == 'timestamp':\n",
        "        data = (\n",
        "            data\n",
        "            .reset_index(level=1, drop=True)\n",
        "            .set_index('RE_DATE', append=True)\n",
        "        )\n",
        "    elif time_form == 'diff':\n",
        "        data = data.drop(columns=['RE_DATE'])\n",
        "\n",
        "    return data\n",
        "\n",
        "################################\n",
        "## Read data functions\n",
        "###############################\n",
        "\n",
        "def read_train_data(path_train):\n",
        "    data_df = pd.read_excel(path_train, encoding='gbk', index_col=[0, 1])  # train_sample_375_v2 train_sample_351_v4\n",
        "    \n",
        "    # Debugging, SS\n",
        "    #with pd.option_context('display.max_rows', 10, 'display.max_columns', 12):  # more options can be specified also\n",
        "    #  print(data_df)\n",
        "    data_df = data_df.groupby('PATIENT_ID').last()\n",
        "    lable = data_df['outcome'].values\n",
        "    data_df = data_df.drop(['outcome', 'Admission time', 'Discharge time'], axis=1)\n",
        "    data_df['Type2'] = lable\n",
        "    data_df = data_df.applymap(lambda x: x.replace('>', '').replace('<', '') if isinstance(x, str) else x)\n",
        "    data_df = data_df.applymap(lambda x: x if is_number(x) else -1)\n",
        "    # data_df = data_df.loc[:, data_df.isnull().mean() < 0.2]\n",
        "    data_df = data_df.astype(float)\n",
        "\n",
        "    return data_df\n",
        "\n",
        "\n",
        "def data_preprocess():\n",
        "    \n",
        "    data_df_unna = read_train_data(train_excel_path)\n",
        "\n",
        "    # Reads the excel as it is, SS\n",
        "    data_pre_df = pd.read_excel(test_excel_path, index_col=[0, 1], encoding='gbk')\n",
        "    #with pd.option_context('display.max_rows', 10, 'display.max_columns', 12):  # more options can be specified also\n",
        "    #  print(\"data_pre_df excel\", data_pre_df)\n",
        "\n",
        "    # Removes NaN, merges rows by filling empty cells form previous rows, SS\n",
        "    # Sorts them so that the latest row (the one with the most amount of data) becomes the first row, SS\n",
        "    data_pre_df = merge_data_by_sliding_window(data_pre_df, n_days=1, dropna=True, subset= top3_feats_cols,\n",
        "                                                     time_form='diff')\n",
        "    #with pd.option_context('display.max_rows', 10, 'display.max_columns', 12):  # more options can be specified also\n",
        "    #  print(\"data_pre_df sliding window\", data_pre_df)\n",
        "\n",
        "    # Creates a new index and chooses the first row (the row that we need only), SS\n",
        "    data_pre_df = data_pre_df.groupby('PATIENT_ID').first().reset_index()\n",
        "    #with pd.option_context('display.max_rows', 10, 'display.max_columns', 12):  # more options can be specified also\n",
        "    #  print(\"data_pre_df sliding group by\", data_pre_df)\n",
        "\n",
        "    # Replaces (<, >) with blank in case that test results came out this way, SS\n",
        "    data_pre_df = data_pre_df.applymap(lambda x: x.replace('>', '').replace('<', '') if isinstance(x, str) else x)\n",
        "    #with pd.option_context('display.max_rows', 10, 'display.max_columns', 12):  # more options can be specified also\n",
        "    #  print(\"data_pre_df sliding applymap\", data_pre_df)\n",
        "\n",
        "    # Removes duplicates just in case if there was duplicates, SS\n",
        "    data_pre_df = data_pre_df.drop_duplicates()\n",
        "    #with pd.option_context('display.max_rows', 10, 'display.max_columns', 12):  # more options can be specified also\n",
        "    #  print(\"data_pre_df sliding drop_dup\", data_pre_df)\n",
        "\n",
        "    return data_df_unna, data_pre_df\n",
        "\n",
        "\n",
        "### is_number in the read data\n",
        "def is_number(s):\n",
        "    if s is None:\n",
        "        s = np.nan\n",
        "\n",
        "    try:\n",
        "        float(s)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        import unicodedata\n",
        "        unicodedata.numeric(s)\n",
        "        return True\n",
        "    except (TypeError, ValueError):\n",
        "        pass\n",
        "\n",
        "    return False\n",
        "\n",
        "### Data read and split\n",
        "def data_read_and_split(is_dropna=False,sub_cols=None):\n",
        "    # data_df_unna is the 375 data set,\n",
        "    # data_pre_df is the 110 data set\n",
        "    data_df_unna,data_pre_df = data_preprocess()\n",
        "    if is_dropna==True:\n",
        "        data_df_unna = data_df_unna.dropna(subset=sub_cols,how='any')\n",
        "\n",
        "    # Calculate the lack of features\n",
        "    col_miss_data = col_miss(data_df_unna)\n",
        "    \n",
        "    # Calculate the proportion of missing features\n",
        "    col_miss_data['Missing_part'] = col_miss_data['missing_count']/len(data_df_unna)\n",
        "\n",
        "    \n",
        "    # Select features that are missing less than 0.2\n",
        "    sel_cols = col_miss_data[col_miss_data['Missing_part']<=0.2]['col']\n",
        "    # The copy function extracts the selected characteristic data...\n",
        "    # without affecting the value of the original data\n",
        "    data_df_sel = data_df_unna[sel_cols].copy()\n",
        "    # Calculate all features\n",
        "    cols = list(data_df_sel.columns)\n",
        "    # get rid of/remove age with gender\n",
        "    cols.remove('age')\n",
        "    cols.remove('gender')\n",
        "    cols.remove('Type2')\n",
        "    cols.append('Type2')\n",
        "\n",
        "    # Debugging, SS\n",
        "    #with pd.option_context('display.max_rows', 10, 'display.max_columns', 12):  # more options can be specified also\n",
        "    #  print(\"data_pre_df sliding window\", data_df_sel)\n",
        "    # Construct a dataframe excluding the above features\n",
        "    data_df_sel2 = data_df_sel[cols]\n",
        "    # Create a new dataframe\n",
        "    data_df_unna = pd.DataFrame()\n",
        "    # Similar to the copy method, new variables...\n",
        "    # and modifications will not affect the original value\n",
        "    data_df_unna = data_df_sel2\n",
        "\n",
        "    # Add -1 to missing values\n",
        "    data_df_unna = data_df_unna.fillna(-1)\n",
        "\n",
        "    # Take out the feature name, from the first column to the penultimate column\n",
        "    x_col = cols[:-1]\n",
        "    \n",
        "    # Remove tag name\n",
        "    y_col = cols[-1]\n",
        "    #Take out 375 feature data\n",
        "    X_data = data_df_unna[x_col]#.values\n",
        "    #Get 375 tag data\n",
        "    Y_data = data_df_unna[y_col]#.values\n",
        "\n",
        "    # Debugging, SS\n",
        "    ## X_data all features, Y_data only Type2 (mortality)\n",
        "    #print(\"X= \",X_data)\n",
        "    #print(\"Y= \",Y_data)\n",
        "    #print(\"x_col = \",x_col)\n",
        "          \n",
        "    return X_data,Y_data,x_col\n",
        "\n",
        "\n",
        "## calculate miss values by col\n",
        "def col_miss(train_df):\n",
        "    col_missing_df = train_df.isnull().sum(axis=0).reset_index()\n",
        "    col_missing_df.columns = ['col','missing_count']\n",
        "    col_missing_df = col_missing_df.sort_values(by='missing_count')\n",
        "    return col_missing_df"
      ],
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm0A-VQvYuxr",
        "colab_type": "text"
      },
      "source": [
        "# Testing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7TLcECNFIes",
        "colab_type": "text"
      },
      "source": [
        "## Single tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPy5wHFVFNf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def single_tree(cols=['LDH','lymph','CRP']):\n",
        "    print('single_tree:\\n')\n",
        "    #Get the data of 375 patients (data_df_unna) and 110 patients (data_pre_df)\n",
        "    data_df_unna,data_pre_df = data_preprocess()\n",
        "    #Remove all blank lines, the total number of 375 now becomes 351\n",
        "    data_df_unna = data_df_unna.dropna(subset=cols,how='any')\n",
        "\n",
        "    cols.append('Type2')\n",
        "    #Get the patient's outcome label\n",
        "    Test_Y = data_pre_df.reset_index()[['serial','PATIENT_ID','outcome']].copy()\n",
        "    #Modify the name of the dataframe\n",
        "    Test_Y = Test_Y.rename(columns={'serial': 'serial', 'PATIENT_ID': 'ID', 'outcome': 'Y'})\n",
        "    \n",
        "    # Get the label data of 110 patients\n",
        "    y_true = Test_Y['Y'].values\n",
        "\n",
        "    x_col = cols[:-1]\n",
        "    #print(x_col)\n",
        "    y_col = cols[-1]\n",
        "    #print(y_col)\n",
        "\n",
        "    # Acquire three characteristic data of 351 patients\n",
        "    x_np = data_df_unna[x_col].values\n",
        "    # Get the tag data of 351 patients\n",
        "    y_np = data_df_unna[y_col].values\n",
        "    # Obtain the three characteristics data of 110 patients\n",
        "    x_test = data_pre_df[x_col].values\n",
        "    # Divide the training set and validation set on 351 patients, at this time 110 is regarded as the test set\n",
        "    X_train, X_val, y_train, y_val = train_test_split(x_np, y_np, test_size=0.3, random_state=6)\n",
        "    \n",
        "    #Limited single tree xgb model\n",
        "    model = xgb.XGBClassifier(\n",
        "        max_depth=3,\n",
        "        n_estimators=1,\n",
        "    )\n",
        "    model.fit(X_train,y_train)\n",
        "    \n",
        "    #Training set confusion matrix\n",
        "    ##pred_train = model.predict(X_train)\n",
        "    ##show_confusion_matrix(y_train, pred_train)\n",
        "    ##print(classification_report(y_train, pred_train))\n",
        "\n",
        "    #Validation set confusion matrix\n",
        "    ##pred_val = model.predict(X_val)\n",
        "    ##show_confusion_matrix(y_val, pred_val)\n",
        "    ##print(classification_report(y_val, pred_val))\n",
        "    \n",
        "    # Model makes predictions, SS\n",
        "    pred_test = model.predict(x_test)\n",
        "    # Convert to pandas dataframe, and output as integars, SS\n",
        "    pred_test = pd.DataFrame(pred_test.astype(int))\n",
        "    # Add the serial number to patients, SS\n",
        "    pred_test.insert(0, \"serial\", Test_Y['serial'], True)\n",
        "    # Rename the outcome column (it was called 0), SS\n",
        "    # Convert to JSON where each index represents one patient, SS\n",
        "    pred_json = pred_test.rename(columns={0: \"predicted\"}).to_json(orient=\"index\")\n",
        "    # Lay out the JSON (load it, don't know what that means), SS\n",
        "    pred_json = json.loads(pred_json)\n",
        "    # Dump it out, indent each patient on a line, SS\n",
        "    pred_json = json.dumps(pred_json)\n",
        "    \n",
        "    return pred_json\n",
        "    #Test set prediction and confusion matrix\n",
        "    ##print('True test label:',y_true)\n",
        "    ##print('Predict test label:',pred_test.astype(int))\n",
        "    ##show_confusion_matrix(y_true, pred_test)\n",
        "    ##print(classification_report(y_true, pred_test))\n",
        "    \n",
        "    # Output as .csv (doesn't work on JSON, only for pd dataframes)\n",
        "    ##pred_test.to_csv(csv_output_path, index = False)\n",
        "    \n",
        "\n",
        "    plt.figure(dpi=300,figsize=(18,6))\n",
        "    plot_tree(model)\n",
        "    plt.show()\n",
        "    \n",
        "    #graph = xgb.to_graphviz(model)\n",
        "    #graph.render(filename='single-tree.dot')\n",
        "    #Single tree visualization\n",
        "    #graph = xgb.to_graphviz(model, fmap='xgb.fmap', num_trees=0, **{'size': str(10)})\n",
        "    #graph.render(filename='single-tree.dot')"
      ],
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6qQLNjAFVTY",
        "colab_type": "text"
      },
      "source": [
        "# Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hfWPV0oFTQs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "c748f408-7016-4aa3-aef2-6e786a9f8244"
      },
      "source": [
        "single_tree()"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "single_tree:\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'{\"0\": {\"serial\": 781743, \"predicted\": 0}, \"1\": {\"serial\": 717333, \"predicted\": 0}, \"2\": {\"serial\": 819070, \"predicted\": 1}, \"3\": {\"serial\": 723839, \"predicted\": 0}, \"4\": {\"serial\": 750040, \"predicted\": 0}, \"5\": {\"serial\": 907451, \"predicted\": 0}, \"6\": {\"serial\": 24923, \"predicted\": 0}, \"7\": {\"serial\": 138806, \"predicted\": 0}, \"8\": {\"serial\": 466866, \"predicted\": 0}, \"9\": {\"serial\": 623147, \"predicted\": 0}, \"10\": {\"serial\": 85314, \"predicted\": 0}, \"11\": {\"serial\": 699834, \"predicted\": 0}, \"12\": {\"serial\": 740546, \"predicted\": 0}, \"13\": {\"serial\": 474336, \"predicted\": 0}, \"14\": {\"serial\": 700168, \"predicted\": 0}, \"15\": {\"serial\": 706945, \"predicted\": 0}, \"16\": {\"serial\": 317983, \"predicted\": 0}, \"17\": {\"serial\": 550915, \"predicted\": 0}, \"18\": {\"serial\": 115584, \"predicted\": 0}, \"19\": {\"serial\": 732451, \"predicted\": 0}, \"20\": {\"serial\": 319737, \"predicted\": 0}, \"21\": {\"serial\": 218836, \"predicted\": 0}, \"22\": {\"serial\": 584423, \"predicted\": 1}, \"23\": {\"serial\": 321499, \"predicted\": 0}, \"24\": {\"serial\": 993466, \"predicted\": 0}, \"25\": {\"serial\": 230666, \"predicted\": 0}, \"26\": {\"serial\": 753514, \"predicted\": 0}, \"27\": {\"serial\": 965199, \"predicted\": 1}, \"28\": {\"serial\": 217212, \"predicted\": 1}, \"29\": {\"serial\": 208758, \"predicted\": 0}, \"30\": {\"serial\": 20548, \"predicted\": 0}, \"31\": {\"serial\": 727649, \"predicted\": 0}, \"32\": {\"serial\": 341016, \"predicted\": 0}, \"33\": {\"serial\": 765095, \"predicted\": 0}, \"34\": {\"serial\": 321583, \"predicted\": 0}, \"35\": {\"serial\": 585032, \"predicted\": 0}, \"36\": {\"serial\": 288270, \"predicted\": 0}, \"37\": {\"serial\": 547170, \"predicted\": 0}, \"38\": {\"serial\": 435525, \"predicted\": 0}, \"39\": {\"serial\": 793297, \"predicted\": 1}, \"40\": {\"serial\": 471616, \"predicted\": 0}, \"41\": {\"serial\": 868982, \"predicted\": 0}, \"42\": {\"serial\": 994718, \"predicted\": 0}, \"43\": {\"serial\": 443045, \"predicted\": 0}, \"44\": {\"serial\": 634323, \"predicted\": 0}, \"45\": {\"serial\": 541189, \"predicted\": 0}, \"46\": {\"serial\": 330274, \"predicted\": 0}, \"47\": {\"serial\": 568400, \"predicted\": 0}, \"48\": {\"serial\": 619211, \"predicted\": 0}, \"49\": {\"serial\": 175196, \"predicted\": 0}, \"50\": {\"serial\": 366742, \"predicted\": 0}, \"51\": {\"serial\": 885053, \"predicted\": 0}, \"52\": {\"serial\": 393855, \"predicted\": 0}, \"53\": {\"serial\": 517100, \"predicted\": 0}, \"54\": {\"serial\": 360766, \"predicted\": 0}, \"55\": {\"serial\": 483792, \"predicted\": 1}, \"56\": {\"serial\": 523761, \"predicted\": 0}, \"57\": {\"serial\": 382952, \"predicted\": 0}, \"58\": {\"serial\": 400577, \"predicted\": 0}, \"59\": {\"serial\": 904473, \"predicted\": 0}, \"60\": {\"serial\": 151357, \"predicted\": 1}, \"61\": {\"serial\": 387094, \"predicted\": 0}, \"62\": {\"serial\": 30225, \"predicted\": 0}, \"63\": {\"serial\": 933415, \"predicted\": 0}, \"64\": {\"serial\": 125519, \"predicted\": 0}, \"65\": {\"serial\": 401540, \"predicted\": 0}, \"66\": {\"serial\": 236443, \"predicted\": 0}, \"67\": {\"serial\": 161860, \"predicted\": 0}, \"68\": {\"serial\": 57084, \"predicted\": 0}, \"69\": {\"serial\": 400615, \"predicted\": 0}, \"70\": {\"serial\": 38571, \"predicted\": 0}, \"71\": {\"serial\": 69848, \"predicted\": 0}, \"72\": {\"serial\": 647183, \"predicted\": 0}, \"73\": {\"serial\": 77088, \"predicted\": 0}, \"74\": {\"serial\": 405477, \"predicted\": 0}, \"75\": {\"serial\": 853578, \"predicted\": 1}, \"76\": {\"serial\": 712986, \"predicted\": 0}, \"77\": {\"serial\": 73257, \"predicted\": 0}, \"78\": {\"serial\": 801989, \"predicted\": 1}, \"79\": {\"serial\": 771184, \"predicted\": 1}, \"80\": {\"serial\": 399167, \"predicted\": 0}, \"81\": {\"serial\": 114766, \"predicted\": 0}, \"82\": {\"serial\": 966932, \"predicted\": 0}, \"83\": {\"serial\": 478483, \"predicted\": 0}, \"84\": {\"serial\": 131970, \"predicted\": 0}, \"85\": {\"serial\": 245398, \"predicted\": 1}, \"86\": {\"serial\": 903796, \"predicted\": 0}, \"87\": {\"serial\": 919550, \"predicted\": 0}, \"88\": {\"serial\": 617101, \"predicted\": 0}, \"89\": {\"serial\": 348750, \"predicted\": 0}, \"90\": {\"serial\": 605600, \"predicted\": 1}, \"91\": {\"serial\": 512850, \"predicted\": 0}, \"92\": {\"serial\": 912276, \"predicted\": 0}, \"93\": {\"serial\": 196998, \"predicted\": 0}, \"94\": {\"serial\": 390823, \"predicted\": 0}, \"95\": {\"serial\": 289597, \"predicted\": 1}, \"96\": {\"serial\": 260768, \"predicted\": 0}, \"97\": {\"serial\": 613344, \"predicted\": 0}, \"98\": {\"serial\": 177709, \"predicted\": 0}, \"99\": {\"serial\": 586311, \"predicted\": 0}, \"100\": {\"serial\": 927828, \"predicted\": 0}, \"101\": {\"serial\": 832543, \"predicted\": 0}, \"102\": {\"serial\": 794543, \"predicted\": 0}, \"103\": {\"serial\": 645344, \"predicted\": 0}, \"104\": {\"serial\": 549744, \"predicted\": 0}, \"105\": {\"serial\": 229929, \"predicted\": 1}, \"106\": {\"serial\": 524368, \"predicted\": 0}, \"107\": {\"serial\": 818335, \"predicted\": 0}, \"108\": {\"serial\": 124429, \"predicted\": 1}, \"109\": {\"serial\": 219120, \"predicted\": 1}}'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 249
        }
      ]
    }
  ]
}